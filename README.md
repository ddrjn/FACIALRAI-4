# FACIALRAI

## This project uses deep-emotion AI to predict Emotion from facial expression

### Emotions used:

* Happy
* Sad 
* Neutral

**Repo is provided with 4 datasets, separately and permutated, precompiled into format that network understands**

* **DDD** - *Dataset of African American people expressing one of three emotions* 
* **ICV mefed**(*here - MEFED*) - *Dataset of white people expressing one of three emotions*
* **Indian** - *Dataset of indian people expressing one of three emotions*
* **Asian** - *Dataset of Asian people expressing one of three emotions*
**! in all datasets gender distribution is close to 50-50**

**Dataset Content**

**DDD**

Dataset was generated by us(Davit Rizhinashvili and Danila Kuklianov). source were images from [Pexels.com](pexels.com) and movies. Python script automatically cut out faces from said sources, images of Black people were chosen and manually classified into emotions.
> Contains 3900 Images in total, of which:

>	>	1900 are Neutral

>   >   1600 are Happy

>   >   420 are Sad

**ICV MEFED**

Dataset of 128 (Mostly white) individuals expressing emotions on deamand. for each emotion multiple images were taken.
> Here used is a subset of Full database. Contains 4700 Images in total, of which:

>	>	320 are Neutral

>   >   2240 are Happy

>   >   2240 are Sad

**Indian**

Dataset was generated by us(Davit Rizhinashvili and Danila Kuklianov). source were images from [Pexels.com](pexels.com). Python script automatically cut out faces from said sources, images of Indian people were chosen and manually classified into emotions.
> Contains 3900 Images in total, of which:

>	>	1196 are Neutral

>   >   1840 are Happy

>   >   908 are Sad

**Asian**

Dataset was generated by us(Davit Rizhinashvili and Danila Kuklianov). source were images from [Pexels.com](pexels.com). Python script automatically cut out faces from said sources, images of Asian people were chosen and manually classified into emotions.
> Contains 3900 Images in total, of which:

>	>	1320 are Neutral

>   >   1434 are Happy

>   >   1146 are Sad


**Common**

First it is very important to stress that style of images chosen is the key. in our experiments we use images of faces that face the camera mostly straight and have no makeup or other things obstracting face (like hands or other objects).

Images are compressed to Grayscale and to size 48X48. All images are face only and were cut out automatically from full images by Face_recoginition library. in csv files, data is split between training and testing sets(split about 75%-25%). images are put in csv files two columns


* First column contains the emotion and is labeled `emotion`

>   * 0 is Happy
>   * 1 is Neutral
>   * 2 is Sad

* Second column contains pixel values as one long String delimited by Space (` `) and is labeled `pixels`

A valid dataset folder from start should contain two csv files: `train.csv` and `test.csv` in manner described, rest will be created automatically.

*! all datasets and combinations of datasets are put into datasets folder*

Datasets are stored in appropriate folders:

* DDD - `D`
* MEFED - `M`
* Indian - `I`
* Asian - `A`

when testing combinations of datasets, we use abbreviations for corresponding dataset, for example folder `IA` contains combination of `Indian` and `Asian` DB's

To prepare the datased and unpack it appropriately, run command 

`python main.py -d datasets/data -s True` *you need to replace "data" with appropriate folder name(DDD,MEFED or Indian... ets.)*

This command will generate necessary files for the network inside the folder you specified.

Then to train, you can run `python main.py -d datasets/data -t True` 

*Again, replace data with appropriate folder name, this will train the network and generate saved state file called `data.pt` where data is replaced with data folder you used.*

**Testing**

Finally you can test out the network accuracy on other images

note that testing data folder has to be structured in simmilar way to how training folders are structured. meaning, it should also contain test.csv and train.csv files and one has to run `python main.py -d datasets/data -s True` command on testing folder as well. 

Once you have testing folder set up , you can call the following command:

`python visualize.py -t True --model data.pt --data datasets/data -tc finaltest`

Here replace `data.pt` with your network saved state filename, `data` with testing folder and `finaltest` with subfolder of your choice on which you want to test the network (*currently data generator class generates 3 subfolders, train,val and finaltest*)

links to used materials:

[deep_emotion](https://github.com/omarsayed7/Deep-Emotion)

[paper](https://arxiv.org/abs/1902.01019)

